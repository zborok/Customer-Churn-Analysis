---
title: "Customer Churn Problem"
author: "Zervaan Borok"
date: "2/20/2022"
output: html_document
---

# Problem Background
Customer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams. Customer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customers are the fuel that powers a business. Loss of customers impacts sales. Further, it’s much more difficult and costly to gain new customers than it is to retain existing customers. As a result, organizations need to focus on reducing customer churn.

 The dataset used for this Keras tutorial is IBM Watson Telco Dataset. According to IBM, the business challenge is:

 "A telecommunications company [Telco] is concerned about the number of           customers leaving their landline business for cable competitors. They           need to understand who is leaving. Imagine that you’re an analyst at            this company and you have to find out who is leaving and why."


We are going to use Keras library to to develop a sophisticated and highly accurate deep learning model in R. We walk you through the preprocessing steps, investing time into how to format the data for Keras.

Finally we show you how to get black box (NN) insighrts using the recently developed lime package.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part 1: Standard Dataframe & Model


```{r}
# Load libraries

library(keras) #modeling NN
library(tensorflow)
set_random_seed(42)
library(tidyverse) #for data manipulation
library(rsample) #for sampling 
library(recipes) #for eficient preprocessing
library(yardstick) #Tidy methods for measuring model performance
library(corrr) #for Correlation Analysis
library(readr)
```

### Read Data
The dataset includes information about:

- Customers who left within the last month: The column is called Churn
- Services that each customer has signed up for: phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
- Customer account information: how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
- Demographic info about customers: gender, age range, and if they have partners and dependents
```{r}
#setwd("YOUR HOME DIRECTORY PATH")
churn_data_raw <- read_csv("C:\\Users\\zerva\\Documents\\AM11\\Telco-Customer-Churn.csv")
head(churn_data_raw)
```


```{r}
df <- churn_data_raw[sample(1:nrow(churn_data_raw)),]

df_1 <- df
df_2 <- df
df_3 <- df
```


### Prune and clean dataset
```{r}
churn_data_tbl <- churn_data_raw %>%
  select(-customerID) %>% # remove the customerID 
  drop_na() %>%  # Drop rows that have NA(Not Available) Values
  select(Churn, everything())

head(churn_data_tbl) #dsiplay 6 first rows of the dataset.
```

### Split data
Split test/training sets using the rsample package
```{r}
set.seed(1000) #for reproducibility
train_test_split <- initial_split(churn_data_tbl, prop = 0.8)
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
cat("Dimensions of the training set is: ", dim(train_tbl), "\n")
cat("Dimensions of the test set is: ", dim(test_tbl), "\n")
```


### Preprocess/Normalize the Data using the friendly "recipe"
1. we discretize the variable *tenure* into 6 categories
2. create the log transformation of *TotalCharges*
3. encode the categorical data into dummy variables
4. to mean-center the data
5. scale the data
6. prepare the recipe, i.e., estimate the required parameters from a training set that can later be applied to other data sets
```{r}
# Create recipe
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl)

# Print the recipe object
rec_obj
```
We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps above. We apply to our training and testing data to convert from raw data to a machine learning dataset.

And finaly, we need to store the actual (truth) values as y_train_vec and y_test_vec, which are needed for training and testing our NN.
```{r}
# Creating the X and Y sets
x_train_tbl <- bake(rec_obj, new_data = train_tbl) %>% select(-Churn)
x_test_tbl  <- bake(rec_obj, new_data = test_tbl) %>% select(-Churn)
glimpse(x_train_tbl)
y_train_vec <- ifelse(pull(train_tbl, Churn) == "Yes", 1, 0)
y_test_vec  <- ifelse(pull(test_tbl, Churn) == "Yes", 1, 0)
```


### Build the NN model

Finally, Deep Learning with Keras in R! 

The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack (sequence) of layers.

*note*: The first layer needs to have the input_shape, that is the numeber of geatures that is getting fed by. In this case it is the number of columns in the x_train_tbl.
```{r}
# Building our Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

#display model architecture
model_keras
```


```{r}
# Train model
history <- fit(
  object           = model_keras, 
  x                = as.matrix(x_train_tbl), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
  ,verbose = 0
)
```


```{r}
# Print a summary of the training history
print(history)
```


```{r}
# Plot the training/validation history of our Keras model
plot(history)
```


let’s make some predictions from our keras model on the test data set, which was unseen during modeling. You can predict *class* or *probability*
```{r}
# Predicted Class
yhat_keras_class_vec <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>% as.vector() %>% `>`(0.5) 
# %>% k_cast("int32") 
    

# Predicted Class Probability
yhat_keras_prob_vec  <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>%
    as.vector()
```


### Inspect Performance With Yardstick
```{r}
# Format test data and predictions for yardstick metrics
# estimates_keras_tbl <- tibble(
#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#   class_prob = yhat_keras_prob_vec
# )
estimates_keras_tbl <- tibble(
  truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "TRUE", no = "FALSE"),
  class_prob = yhat_keras_prob_vec
)
options(yardstick.event_first = FALSE) # the default is to classify 0 as the positive class instead of 1
estimates_keras_tbl
```


### Confusion Table
```{r}
estimates_keras_tbl %>% conf_mat(truth, estimate)
```


### Accuracy
```{r}
acc_0 <- estimates_keras_tbl %>% accuracy(truth, estimate)
```


### AUC
ROC Area Under the Curve (AUC) measurement
```{r}
roc_0 <- estimates_keras_tbl %>% roc_auc(truth, class_prob)
```


### Precision and Recall
Precision is when the model predicts “yes”, how often is it actually “yes”.
Recall (also true positive rate) is when the actual value is “yes” how often is the model correct
```{r}
prec_0 <- estimates_keras_tbl %>% precision(truth, estimate)
rec_0 <- estimates_keras_tbl %>% recall(truth, estimate)
```


### F1 Score
weighted average between the precision and recal
```{r}
fmeas_0 <- estimates_keras_tbl %>% f_meas(truth, estimate)
```


```{r}
metrics_0 <- rbind(acc_0, roc_0, prec_0, rec_0, fmeas_0)
metrics_0 
```



# Part 2: Imputation 


## Add in 35% missing values and drop NA rows


### Prune and clean dataset
```{r}
df_1 <- df_1 %>%
  select(-customerID) %>% # remove the customerID 
  drop_na() %>%
  select(Churn, everything())

head(df_1) #dsiplay 6 first rows of the dataset.
```


```{r}
df_1$tenure[1:2461] <- NA
df_1$MonthlyCharges[1:2461] <- NA
df_1$TotalCharges[1:2461] <- NA
head(df_1)
```


```{r}
df_1 <- df_1 %>% 
  drop_na() %>%
  select(Churn, everything())

head(df_1)
```


### Split data
Split test/training sets using the rsample package
```{r}
set.seed(1000) #for reproducibility
train_test_split_1 <- initial_split(df_1, prop = 0.8)
train_tbl_1 <- training(train_test_split_1)
test_tbl_1  <- testing(train_test_split_1)
cat("Dimensions of the training set is: ", dim(train_tbl_1), "\n")
cat("Dimensions of the test set is: ", dim(test_tbl_1), "\n")
```


```{r}
# Create recipe
rec_obj_1 <- recipe(Churn ~ ., data = train_tbl_1) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl_1)

# Print the recipe object
rec_obj_1
```
We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps above. We apply to our training and testing data to convert from raw data to a machine learning dataset.

And finaly, we need to store the actual (truth) values as y_train_vec and y_test_vec, which are needed for training and testing our NN.
```{r}
# Creating the X and Y sets
x_train_tbl_1 <- bake(rec_obj_1, new_data = train_tbl_1) %>% select(-Churn)
x_test_tbl_1  <- bake(rec_obj_1, new_data = test_tbl_1) %>% select(-Churn)
glimpse(x_train_tbl_1)
y_train_vec_1 <- ifelse(pull(train_tbl_1, Churn) == "Yes", 1, 0)
y_test_vec_1  <- ifelse(pull(test_tbl_1, Churn) == "Yes", 1, 0)
```


### Build the NN model

Finally, Deep Learning with Keras in R! 

The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack (sequence) of layers.

*note*: The first layer needs to have the input_shape, that is the numeber of geatures that is getting fed by. In this case it is the number of columns in the x_train_tbl.
```{r}
# Building our Artificial Neural Network
model_keras_1 <- keras_model_sequential()

model_keras_1 %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl_1)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

#display model architecture
model_keras_1
```


```{r}
# Train model
history_1 <- fit(
  object           = model_keras_1, 
  x                = as.matrix(x_train_tbl_1), 
  y                = y_train_vec_1,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
  ,verbose = 0
)
```


```{r}
# Print a summary of the training history
print(history_1)
```


```{r}
# Plot the training/validation history of our Keras model
plot(history_1)
```
*Tip* Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.


let’s make some predictions from our keras model on the test data set, which was unseen during modeling. You can predict *class* or *probability*
```{r}
# Predicted Class
yhat_keras_class_vec_1 <- predict(object = model_keras_1, x = as.matrix(x_test_tbl_1)) %>% as.vector() %>% `>`(0.5) 
# %>% k_cast("int32") 
    

# Predicted Class Probability
yhat_keras_prob_vec_1  <- predict(object = model_keras_1, x = as.matrix(x_test_tbl_1)) %>%
    as.vector()
```


### Inspect Performance With Yardstick
```{r}
# Format test data and predictions for yardstick metrics
# estimates_keras_tbl <- tibble(
#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#   class_prob = yhat_keras_prob_vec
# )
estimates_keras_tbl_1 <- tibble(
  truth      = as.factor(y_test_vec_1) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec_1) %>% fct_recode(yes = "TRUE", no = "FALSE"),
  class_prob = yhat_keras_prob_vec_1
)
options(yardstick.event_first = FALSE) # the default is to classify 0 as the positive class instead of 1
estimates_keras_tbl_1
```


## Add in 35% missing values and replace with column means


### Prune and clean dataset
```{r}
df_2 <- df_2 %>%
  select(-customerID) %>% # remove the customerID 
  drop_na() %>%
  select(Churn, everything())

head(df_2) #dsiplay 6 first rows of the dataset.
```


```{r}
df_2$tenure[1:2461] <- NA
df_2$MonthlyCharges[1:2461] <- NA
df_2$TotalCharges[1:2461] <- NA
head(df_2)
```


```{r}
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.na))

df_2$tenure[is.na(df_2$tenure)] <- mean(df_2$tenure[2462:7032])
df_2$MonthlyCharges[is.na(df_2$MonthlyCharges)] <- mean(df_2$MonthlyCharges[2462:7032])
df_2$TotalCharges[is.na(df_2$TotalCharges)] <- mean(df_2$TotalCharges[2462:7032])

head(df_2)
```


### Split data
Split test/training sets using the rsample package
```{r}
set.seed(1000) #for reproducibility
train_test_split_2 <- initial_split(df_2, prop = 0.8)
train_tbl_2 <- training(train_test_split_2)
test_tbl_2  <- testing(train_test_split_2)
cat("Dimensions of the training set is: ", dim(train_tbl_2), "\n")
cat("Dimensions of the test set is: ", dim(test_tbl_2), "\n")
```


```{r}
# Create recipe
rec_obj_2 <- recipe(Churn ~ ., data = train_tbl_2) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl_2)

# Print the recipe object
rec_obj_2
```
We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps above. We apply to our training and testing data to convert from raw data to a machine learning dataset.

And finaly, we need to store the actual (truth) values as y_train_vec and y_test_vec, which are needed for training and testing our NN.
```{r}
# Creating the X and Y sets
x_train_tbl_2 <- bake(rec_obj_2, new_data = train_tbl_2) %>% select(-Churn)
x_test_tbl_2  <- bake(rec_obj_2, new_data = test_tbl_2) %>% select(-Churn)
glimpse(x_train_tbl_2)
y_train_vec_2 <- ifelse(pull(train_tbl_2, Churn) == "Yes", 1, 0)
y_test_vec_2  <- ifelse(pull(test_tbl_2, Churn) == "Yes", 1, 0)
```


### Build the NN model

Finally, Deep Learning with Keras in R! 

The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack (sequence) of layers.

*note*: The first layer needs to have the input_shape, that is the numeber of geatures that is getting fed by. In this case it is the number of columns in the x_train_tbl.
```{r}
# Building our Artificial Neural Network
model_keras_2 <- keras_model_sequential()

model_keras_2 %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl_2)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

#display model architecture
model_keras_2
```


```{r}
# Train model
history_2 <- fit(
  object           = model_keras_2, 
  x                = as.matrix(x_train_tbl_2), 
  y                = y_train_vec_2,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
  ,verbose = 0
)
```


```{r}
# Print a summary of the training history
print(history_2)
```


```{r}
# Plot the training/validation history of our Keras model
plot(history_2)
```
*Tip* Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.


let’s make some predictions from our keras model on the test data set, which was unseen during modeling. You can predict *class* or *probability*
```{r}
# Predicted Class
yhat_keras_class_vec_2 <- predict(object = model_keras_2, x = as.matrix(x_test_tbl_2)) %>% as.vector() %>% `>`(0.5) 
# %>% k_cast("int32") 
    

# Predicted Class Probability
yhat_keras_prob_vec_2  <- predict(object = model_keras_2, x = as.matrix(x_test_tbl_2)) %>%
    as.vector()
```


### Inspect Performance With Yardstick
```{r}
# Format test data and predictions for yardstick metrics
# estimates_keras_tbl <- tibble(
#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#   class_prob = yhat_keras_prob_vec
# )
estimates_keras_tbl_2 <- tibble(
  truth      = as.factor(y_test_vec_2) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec_2) %>% fct_recode(yes = "TRUE", no = "FALSE"),
  class_prob = yhat_keras_prob_vec_2
)
options(yardstick.event_first = FALSE) # the default is to classify 0 as the positive class instead of 1
estimates_keras_tbl_2
```


## Add in 35% missing values and replace with column medians


### Prune and clean dataset
```{r}
df_3 <- df_3 %>%
  select(-customerID) %>% # remove the customerID
  drop_na() %>%
  select(Churn, everything())

head(df_3) #dsiplay 6 first rows of the dataset.
```


```{r}
df_3$tenure[1:2461] <- NA
df_3$MonthlyCharges[1:2461] <- NA
df_3$TotalCharges[1:2461] <- NA
head(df_3)
```


```{r}
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.na))

df_3$tenure[is.na(df_3$tenure)] <- median(df_3$tenure[2462:7032])
df_3$MonthlyCharges[is.na(df_3$MonthlyCharges)] <- median(df_3$MonthlyCharges[2462:7032])
df_3$TotalCharges[is.na(df_3$TotalCharges)] <- median(df_3$TotalCharges[2462:7032])

head(df_3)
```


### Split data
Split test/training sets using the rsample package
```{r}
set.seed(1000) #for reproducibility
train_test_split_3 <- initial_split(df_3, prop = 0.8)
train_tbl_3 <- training(train_test_split_3)
test_tbl_3  <- testing(train_test_split_3)
cat("Dimensions of the training set is: ", dim(train_tbl_3), "\n")
cat("Dimensions of the test set is: ", dim(test_tbl_3), "\n")
```


### Preprocess/Normalize the Data using the friendly "recipe"
```{r}
# Create recipe
rec_obj_3 <- recipe(Churn ~ ., data = train_tbl_3) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl_3)

# Print the recipe object
rec_obj_3
```
We can apply the “recipe” to any data set with the bake() function, and it processes the data following our recipe steps above. We apply to our training and testing data to convert from raw data to a machine learning dataset.

And finaly, we need to store the actual (truth) values as y_train_vec and y_test_vec, which are needed for training and testing our NN.
```{r}
# Creating the X and Y sets
x_train_tbl_3 <- bake(rec_obj_3, new_data = train_tbl_3) %>% select(-Churn)
x_test_tbl_3  <- bake(rec_obj_3, new_data = test_tbl_3) %>% select(-Churn)
glimpse(x_train_tbl_3)
y_train_vec_3 <- ifelse(pull(train_tbl_3, Churn) == "Yes", 1, 0)
y_test_vec_3  <- ifelse(pull(test_tbl_3, Churn) == "Yes", 1, 0)
```


### Build the NN model

Finally, Deep Learning with Keras in R! 

The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack (sequence) of layers.

*note*: The first layer needs to have the input_shape, that is the numeber of geatures that is getting fed by. In this case it is the number of columns in the x_train_tbl.
```{r}
# Building our Artificial Neural Network
model_keras_3 <- keras_model_sequential()

model_keras_3 %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl_3)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

#display model architecture
model_keras_3
```


```{r}
# Train model
history_3 <- fit(
  object           = model_keras_3, 
  x                = as.matrix(x_train_tbl_3), 
  y                = y_train_vec_3,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
  ,verbose = 0
)
```


```{r}
# Print a summary of the training history
print(history_3)
```


```{r}
# Plot the training/validation history of our Keras model
plot(history_3)
```
*Tip* Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.


let’s make some predictions from our keras model on the test data set, which was unseen during modeling. You can predict *class* or *probability*
```{r}
# Predicted Class
yhat_keras_class_vec_3 <- predict(object = model_keras_3, x = as.matrix(x_test_tbl_3)) %>% as.vector() %>% `>`(0.5) 
# %>% k_cast("int32") 
    

# Predicted Class Probability
yhat_keras_prob_vec_3  <- predict(object = model_keras_3, x = as.matrix(x_test_tbl_3)) %>%
    as.vector()
```


### Inspect Performance With Yardstick
```{r}
# Format test data and predictions for yardstick metrics
# estimates_keras_tbl <- tibble(
#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#   class_prob = yhat_keras_prob_vec
# )
estimates_keras_tbl_3 <- tibble(
  truth      = as.factor(y_test_vec_3) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec_3) %>% fct_recode(yes = "TRUE", no = "FALSE"),
  class_prob = yhat_keras_prob_vec_3
)
options(yardstick.event_first = FALSE) # the default is to classify 0 as the positive class instead of 1
estimates_keras_tbl_3
```


### Confusion Table
```{r}
estimates_keras_tbl_1 %>% conf_mat(truth, estimate)
```


### Accuracy
```{r}
acc_1 <- estimates_keras_tbl_1 %>% accuracy(truth, estimate)
```


### AUC
ROC Area Under the Curve (AUC) measurement
```{r}
roc_1 <- estimates_keras_tbl_1 %>% roc_auc(truth, class_prob)
```


### Precision and Recall
Precision is when the model predicts “yes”, how often is it actually “yes”.
Recall (also true positive rate) is when the actual value is “yes” how often is the model correct
```{r}
prec_1 <- estimates_keras_tbl_1 %>% precision(truth, estimate)
rec_1 <- estimates_keras_tbl_1 %>% recall(truth, estimate)
```


### F1 Score
weighted average between the precision and recal
```{r}
fmeas_1 <- estimates_keras_tbl_1 %>% f_meas(truth, estimate)
```


```{r}
metrics_1 <- rbind(acc_1, roc_1, prec_1, rec_1, fmeas_1)
metrics_1 <- metrics_1 %>%
  add_column(Model = c('35% Removed', '35% Removed', '35% Removed', '35% Removed', '35% Removed'), .before=".metric")
metrics_1
```


### Confusion Table
```{r}
estimates_keras_tbl_2 %>% conf_mat(truth, estimate)
```


### Accuracy
```{r}
acc_2 <- estimates_keras_tbl_2 %>% accuracy(truth, estimate)
```


### AUC
ROC Area Under the Curve (AUC) measurement
```{r}
roc_2 <- estimates_keras_tbl_2 %>% roc_auc(truth, class_prob)
```


### Precision and Recall
Precision is when the model predicts “yes”, how often is it actually “yes”.
Recall (also true positive rate) is when the actual value is “yes” how often is the model correct
```{r}
prec_2 <- estimates_keras_tbl_2 %>% precision(truth, estimate)
rec_2 <- estimates_keras_tbl_2 %>% recall(truth, estimate)
```


### F1 Score
weighted average between the precision and recal
```{r}
fmeas_2 <- estimates_keras_tbl_2 %>% f_meas(truth, estimate)
```


```{r}
metrics_2 <- rbind(acc_2, roc_2, prec_2, rec_2, fmeas_2)
metrics_2 <- metrics_2 %>%
  add_column(Model = c('35% Mean Imputation', '35% Mean Imputation', '35% Mean Imputation', '35% Mean Imputation', '35% Mean Imputation'), .before=".metric")
metrics_2
```


### Confusion Table
```{r}
estimates_keras_tbl_3 %>% conf_mat(truth, estimate)
```


### Accuracy
```{r}
acc_3 <- estimates_keras_tbl_3 %>% accuracy(truth, estimate)
```


### AUC
ROC Area Under the Curve (AUC) measurement
```{r}
roc_3 <- estimates_keras_tbl_3 %>% roc_auc(truth, class_prob)
```


### Precision and Recall
Precision is when the model predicts “yes”, how often is it actually “yes”.
Recall (also true positive rate) is when the actual value is “yes” how often is the model correct
```{r}
prec_3 <- estimates_keras_tbl_3 %>% precision(truth, estimate)
rec_3 <- estimates_keras_tbl_3 %>% recall(truth, estimate)
```


### F1 Score
weighted average between the precision and recal
```{r}
fmeas_3 <- estimates_keras_tbl_3 %>% f_meas(truth, estimate)
```


```{r}
metrics_3 <- rbind(acc_3, roc_3, prec_3, rec_3, fmeas_3)
metrics_3 <- metrics_3 %>%
  add_column(Model = c('35% Median Imputation', '35% Median Imputation', '35% Median Imputation', '35% Median Imputation', '35% Median Imputation'), .before=".metric")
metrics_3
```


```{r}
model_metrics_combined <- rbind(metrics_1, metrics_2, metrics_3)
names(model_metrics_combined)[names(model_metrics_combined) == ".metric"] <- "Metric"
names(model_metrics_combined)[names(model_metrics_combined) == ".estimator"] <- "Estimator"
names(model_metrics_combined)[names(model_metrics_combined) == ".estimate"] <- "Estimate"

model_metrics_combined <- model_metrics_combined %>%
                            add_row(Model = NA, Metric = NA, Estimator = NA, Estimate = NA, .before = 6) %>%
                            add_row(Model = NA, Metric = NA, Estimator = NA, Estimate = NA, .before = 12) 

model_metrics_combined$Model[is.na(model_metrics_combined$Model)] <- "--"
model_metrics_combined$Metric[is.na(model_metrics_combined$Metric)] <- "--"
model_metrics_combined$Estimator[is.na(model_metrics_combined$Estimator)] <- "--"
model_metrics_combined$Estimate[is.na(model_metrics_combined$Estimate)] <- "--"

model_metrics_combined
```




# Hyperparameter Tuning


```{r}
FLAGS <- flags(
  flag_numeric('dropout1', 0.1),
  flag_integer('neurons1', 16),
  flag_numeric('dropout2', 0.1),
  flag_integer('neurons2', 16)
)

```


```{r}
# Building our Artificial Neural Network

model_keras_tuned_5 <- keras_model_sequential()
  
model_keras_tuned_5 %>%
  # First hidden layer
  layer_dense(
    units              = FLAGS$neurons1, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = FLAGS$dropout1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = FLAGS$neurons2, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = FLAGS$dropout2) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )
history_tuned_5 <- fit(
  object           = model_keras_tuned_5, 
  x                = as.matrix(x_train_tbl), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30, #to include 30% of the data for model validation, which prevents overfitting.
  verbose = 0
)

#display model architecture
model_keras_tuned_5
```


```{r}
library(tfruns)
par <- list(dropout1 = c(0.1, 0.01, 0.001), neurons1 = c(16, 32, 64), dropout2 = c(0.1, 0.01, 0.001), neurons2 = c(16, 32, 64))

runs <- tuning_run('ANN_M1.R', flags = par)

runs <- runs[order(-runs$metric_val_accuracy),]
head(runs)
```


```{r}
runs[1,6:9]
```


## Using Optimal Parameters
```{r}
# Creating the X and Y sets
x_train_tbl_6 <- x_train_tbl
x_test_tbl_6  <- x_test_tbl
y_train_vec_6 <- y_train_vec
y_test_vec_6  <- y_test_vec
```


### Build the NN model

Finally, Deep Learning with Keras in R! 

The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack (sequence) of layers.

*note*: The first layer needs to have the input_shape, that is the numeber of geatures that is getting fed by. In this case it is the number of columns in the x_train_tbl.
```{r}
# Building our Artificial Neural Network
model_keras_6 <- keras_model_sequential()

model_keras_6 %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu", 
    input_shape        = ncol(x_train_tbl_6)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.001) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "relu") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "sigmoid") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss      = 'binary_crossentropy',
    metrics   = c('accuracy')
  )

#display model architecture
model_keras_6
```


```{r}
# Train model
history_6 <- fit(
  object           = model_keras_6, 
  x                = as.matrix(x_train_tbl_6), 
  y                = y_train_vec_6,
  batch_size       = 50, 
  epochs           = 35,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
  ,verbose = 0
)
```


```{r}
# Print a summary of the training history
print(history_6)
```


```{r}
# Plot the training/validation history of our Keras model
plot(history_6)
```
*Tip* Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.


let’s make some predictions from our keras model on the test data set, which was unseen during modeling. You can predict *class* or *probability*
```{r}
# Predicted Class
yhat_keras_class_vec_6 <- predict(object = model_keras_6, x = as.matrix(x_test_tbl_6)) %>% as.vector() %>% `>`(0.5) 
# %>% k_cast("int32") 
    

# Predicted Class Probability
yhat_keras_prob_vec_6  <- predict(object = model_keras_6, x = as.matrix(x_test_tbl_6)) %>%
    as.vector()
```


### Inspect Performance With Yardstick
```{r}
# Format test data and predictions for yardstick metrics
# estimates_keras_tbl <- tibble(
#   truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#   estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#   class_prob = yhat_keras_prob_vec
# )
estimates_keras_tbl_6 <- tibble(
  truth      = as.factor(y_test_vec_6) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec_6) %>% fct_recode(yes = "TRUE", no = "FALSE"),
  class_prob = yhat_keras_prob_vec_6
)
options(yardstick.event_first = FALSE) # the default is to classify 0 as the positive class instead of 1
estimates_keras_tbl_6
```


### Confusion Table
```{r}
estimates_keras_tbl_6 %>% conf_mat(truth, estimate)
```


### Accuracy
```{r}
acc_6 <- estimates_keras_tbl_6 %>% accuracy(truth, estimate)
```


### AUC
ROC Area Under the Curve (AUC) measurement
```{r}
roc_6 <- estimates_keras_tbl_6 %>% roc_auc(truth, class_prob)
```


### Precision and Recall
Precision is when the model predicts “yes”, how often is it actually “yes”.
Recall (also true positive rate) is when the actual value is “yes” how often is the model correct
```{r}
prec_6 <- estimates_keras_tbl_6 %>% precision(truth, estimate)
rec_6 <- estimates_keras_tbl_6 %>% recall(truth, estimate)
```


### F1 Score
weighted average between the precision and recal
```{r}
fmeas_6 <- estimates_keras_tbl_6 %>% f_meas(truth, estimate)
```


```{r}
metrics_6 <- rbind(acc_6, roc_6, prec_6, rec_6, fmeas_6)
metrics_6 <- metrics_6 %>% select(c(1,3))
metrics_6 <- cbind(Model = c("Tuned ANN", "Tuned ANN", "Tuned ANN", "Tuned ANN", "Tuned ANN"), metrics_6)
metrics_6
```




# KNN Model


```{r}
train_tbl_4 <- train_tbl
#train_tbl_4$Churn[train_tbl_4$Churn == "Yes"] <- 1
#train_tbl_4$Churn[train_tbl_4$Churn == "No"] <- 0

test_tbl_4 <- test_tbl
#test_tbl_4$Churn[test_tbl_4$Churn == "Yes"] <- 1
#test_tbl_4$Churn[test_tbl_4$Churn == "No"] <- 0
```


```{r}
library(caret)
knn_fit <- train(Churn ~., data = train_tbl_4, method = "knn", trControl = trainControl("cv", classProbs = TRUE, summaryFunction = twoClassSummary, number = 10), preProcess = c("center", "scale"), metric = "ROC")
knn_fit
```


```{r}
predictions_knn <- predict(knn_fit, test_tbl_4)

table(predictions_knn, test_tbl_4$Churn)
```


```{r}
knn_roc <- as.data.frame(knn_fit$results[3,2])

knn_roc["Metric"] <- "roc_auc"

knn_roc <- knn_roc[, c(2,1)]

names(knn_roc)[names(knn_roc) == "knn_fit$results[3, 2]"] <- "Estimate"

knn_acc <- (table(predictions_knn, test_tbl_4$Churn)[1] + table(predictions_knn, test_tbl_4$Churn)[4]) / 
     (table(predictions_knn, test_tbl_4$Churn)[1] + table(predictions_knn, test_tbl_4$Churn)[2] + table(predictions_knn, test_tbl_4$Churn)[3] + table(predictions_knn, test_tbl_4$Churn)[4])

knn_prec <- table(predictions_knn, test_tbl_4$Churn)[4]/(table(predictions_knn, test_tbl_4$Churn)[3] + table(predictions_knn, test_tbl_4$Churn)[4])

knn_rec <- table(predictions_knn, test_tbl_4$Churn)[4]/(table(predictions_knn, test_tbl_4$Churn)[2] + table(predictions_knn, test_tbl_4$Churn)[4])

knn_fmeas <- (2 * knn_prec * knn_rec)/(knn_prec + knn_rec)

metrics_knn <- as.data.frame(rbind(knn_acc, knn_prec, knn_rec, knn_fmeas))
metrics_knn <- cbind(Metric = rownames(metrics_knn), metrics_knn)
rownames(metrics_knn) <- 1:nrow(metrics_knn)
names(metrics_knn)[names(metrics_knn) == "V1"] <- "Estimate"
metrics_knn <- rbind(metrics_knn, knn_roc)
metrics_knn <- metrics_knn[c(1,5,2,3,4),]
metrics_knn$Metric[metrics_knn$Metric == "knn_acc"] <- "accuracy"
metrics_knn$Metric[metrics_knn$Metric == "knn_prec"] <- "precision"
metrics_knn$Metric[metrics_knn$Metric == "knn_rec"] <- "recall"
metrics_knn$Metric[metrics_knn$Metric == "knn_fmeas"] <- "f_meas"
metrics_knn <- cbind(Model = c("KNN", "KNN", "KNN", "KNN", "KNN"), metrics_knn)


metrics_knn
```


```{r}
metrics_0 <- metrics_0 %>% select(c(1,3))
metrics_0 <- cbind(Model = c("Untuned ANN", "Untuned ANN", "Untuned ANN", "Untuned ANN", "Untuned ANN"), metrics_0)
metrics_0

names(metrics_0)[names(metrics_0) == ".metric"] <- "Metric"
names(metrics_0)[names(metrics_0) == ".estimate"] <- "Estimate"

names(metrics_6)[names(metrics_6) == ".metric"] <- "Metric"
names(metrics_6)[names(metrics_6) == ".estimate"] <- "Estimate"

model_metrics_combined_2 <- rbind(metrics_0, metrics_6, metrics_knn)

model_metrics_combined_2 <- model_metrics_combined_2 %>%
                            add_row(Model = NA, Metric = NA, Estimate = NA, .before = 6) %>%
                            add_row(Model = NA, Metric = NA, Estimate = NA, .before = 12) 

model_metrics_combined_2$Model[is.na(model_metrics_combined_2$Model)] <- "--"
model_metrics_combined_2$Metric[is.na(model_metrics_combined_2$Metric)] <- "--"
model_metrics_combined_2$Estimate[is.na(model_metrics_combined_2$Estimate)] <- "--"

model_metrics_combined_2
```